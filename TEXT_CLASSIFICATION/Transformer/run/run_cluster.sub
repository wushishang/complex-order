#!/bin/bash
#SBATCH -A partner
#SBATCH -t 24:00:00
#SBATCH --job-name=worker
#SBATCH -o worker_%A_%a.out
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --array=0-40%8

echo "$HOSTNAME"
echo "$SLURM_JOB_NODELIST"
echo "$CUDA_VISIBLE_DEVICES"
# For conda to work
#. /apps/cent7/anaconda/5.1.0-py36/etc/profile.d/conda.sh    # I may need to change this to 5.1.0-py36
#source activate py37
srun python -u $PYTHON_FILE instantiate_workers --start 0 --procs $N_PROCS --single --cluster --gpu_spec "0"


##########
# sbatch --export=ALL,PYTHON_FILE=run_experiments.py,N_PROCS=4 run/run_cluster.sub
# sbatch --array=0-30%6 --export=ALL,MKL_THREADING_LAYER=GNU,MKL_NUM_THREADS=1,PYTHON_FILE=run_experiments.py,N_PROCS=1 run/run_cluster.sub
# sbatch --gpus-per-task=1

# To avoid zombie workers on Gilbreth, specify types of nodes that are verified working when submitting jobs, e.g.,
# sbatch --constraint=E  --array=0-6%6 --export=ALL,MKL_THREADING_LAYER=GNU,MKL_NUM_THREADS=2,PYTHON_FILE=run_experiments.py,N_PROCS=4 --gpus-per-task=1 run/run_cluster.sub

# On Bruno's machines, remember to specify -p and --cpus-per-task
# sbatch (-p ml08-gpu) --array=0-4%4 --export=ALL,MKL_THREADING_LAYER=GNU,MKL_NUM_THREADS=3,PYTHON_FILE=run_experiments.py,N_PROCS=4 --gpus-per-task=1 (--cpus-per-task=12) run/run_cluster.sub